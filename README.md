# ğŸ§  GPT-2 Text Generation with Custom Dataset

This project showcases how to fine-tune the GPT-2 language model using a custom dataset for text generation tasks. The training and evaluation were performed on Google Colab with the help of Hugging Face's `transformers` library.

---

## ğŸ“Œ Project Highlights

- âœ… Fine-tuned GPT-2 using a custom dataset
- âœ… Trained entirely in Google Colab using GPU
- âœ… Saved and reloaded the model using Hugging Face tools
- âœ… Uploaded the model to Hugging Face Hub for easy access
- âœ… Used Transformers and PyTorch under the hood

---

## ğŸ› ï¸ Technologies Used

- Python
- PyTorch
- Hugging Face Transformers
- Google Colab
- Hugging Face Hub
- GitHub

---

## ğŸ“ Repository Structure

| File / Folder              | Description                                               |
|---------------------------|-----------------------------------------------------------|
| `Text_Generation_GPT2.ipynb` | Main Colab notebook with full training + generation flow |
| `input.txt`                | Custom dataset used for fine-tuning                      |
| `README.md`                | This file                                                |

---

## ğŸ“Š Dataset

You can use any plain text dataset. In this project, a `.txt` file with custom sentences was used.
