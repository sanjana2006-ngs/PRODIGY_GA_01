# 🧠 GPT-2 Text Generation with Custom Dataset

This project showcases how to fine-tune the GPT-2 language model using a custom dataset for text generation tasks. The training and evaluation were performed on Google Colab with the help of Hugging Face's `transformers` library.

---

## 📌 Project Highlights

- ✅ Fine-tuned GPT-2 using a custom dataset
- ✅ Trained entirely in Google Colab using GPU
- ✅ Saved and reloaded the model using Hugging Face tools
- ✅ Uploaded the model to Hugging Face Hub for easy access
- ✅ Used Transformers and PyTorch under the hood

---

## 🛠️ Technologies Used

- Python
- PyTorch
- Hugging Face Transformers
- Google Colab
- Hugging Face Hub
- GitHub

---

## 📁 Repository Structure

| File / Folder              | Description                                               |
|---------------------------|-----------------------------------------------------------|
| `Text_Generation_GPT2.ipynb` | Main Colab notebook with full training + generation flow |
| `input.txt`                | Custom dataset used for fine-tuning                      |
| `README.md`                | This file                                                |

---

## 📊 Dataset

You can use any plain text dataset. In this project, a `.txt` file with custom sentences was used.
