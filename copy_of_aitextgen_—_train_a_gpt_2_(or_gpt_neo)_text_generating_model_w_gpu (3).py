# -*- coding: utf-8 -*-
"""Copy of aitextgen â€” Train a GPT-2 (or GPT Neo) Text-Generating Model w/ GPU

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19lP2fLdvd_ADnUtGBgzlYOwTgBjfNk37

# Text Generation with GPT-2  
ðŸ”— **Project GitHub Repository:** [PRODIGY_GA_01](https://github.com/sanjana2006-ngs/PRODIGY_GA_01)

This notebook is created as part of a Prodigy Infotech internship task. It demonstrates how to train a custom text generation model using `aitextgen` with free GPU support on Google Colab.

Model: GPT-2  
Libraries: [`transformers`](https://huggingface.co/docs/transformers)  
Platform: Google Colab

---

**Submitted by:** Gayathri Sanjana Nukala  
**Start Date:** June 15, 2025
"""

import logging,torch,os
import streamlit as st
logging.basicConfig(
    format="%(asctime)s â€” %(levelname)s â€” %(name)s â€” %(message)s",
    datefmt="%m/%d/%Y %H:%M:%S",
    level=logging.INFO
)

# Load GPT-2 text generation pipeline
from transformers import pipeline,TextDataset,GPT2LMHeadModel, GPT2Tokenizer

generator = pipeline("text-generation", model="gpt2")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

"""##Loading GPT-2 Model
In this step, I am loading the GPT-2 model (124M) into the Colab environment using the transformers library from Hugging Face. This model is a smaller version of GPT-2 and is suitable for basic text generation tasks.

The pipeline API from Hugging Face makes it easy to load and use pre-trained models. Once loaded, it can be used for generating text in just a few lines of code.

Now, I will load the GPT-2 124M model using the pipeline as shown below:
"""

generator = pipeline("text-generation", model="gpt2")

"""##Mounting Google Drive
To upload input text files or save generated outputs, I need to connect my Google Drive to Colab.
This allows me to easily move files between my local storage and the notebook.

When I run the code, it will ask for an authorization code â€” itâ€™s safe and temporary.
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Uploading a Text File to Colab
In this step, Iâ€™m uploading the text file that I will use to train my GPT-2 model.

To do this, I open the Files tab in the left sidebar of Google Colab and upload a .txt file from my local system.

This file contains the custom text data that I want the model to learn from.
If the file is too large, I can trim it using Python to make the training process faster and more efficient.

Once uploaded, the file will appear in the sidebar, and I will use it in the next steps for tokenization and training.
"""

file_path = "/content/drive/MyDrive/Colab_Notebooks/input.txt"

"""## Finetuning GPT-2 on My Custom Text Dataset

In this step, I will fine-tune the GPT-2 model using a custom text file. The model will be trained to learn patterns from my dataset and generate similar text.

I will monitor the training progress through loss values to ensure the model is learning effectively. The model will be saved periodically to avoid any data loss during the training process.

If Google Drive is mounted, a backup of the model will also be saved there.

### Key Parameters:
- **`num_train_epochs`**: Number of times the model will go through the entire dataset.
- **`per_device_train_batch_size`**: Number of samples per training step per GPU.
- **`save_steps`**: Interval (in steps) to save the model.
- **`logging_steps`**: Frequency of logging training loss.
- **`fp16`**: Enable mixed precision training for faster performance on supported GPUs (e.g., T4, V100).
- **`output_dir`**: Directory where the trained model will be saved.
- **`push_to_hub`**: Set to `False` unless you want to push your model to Hugging Face Hub.

After setting these configurations, I will start the training using `Trainer` from the `transformers` library.

"""

dataset = TextDataset(tokenizer=tokenizer,file_path=file_path, block_size=128)

"""## Save the Trained Model and Tokenizer

Once training is complete, save both the model and the tokenizer so they can be reused later without retraining.

"""

model = GPT2LMHeadModel.from_pretrained("gpt2")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

model_path = "/content/drive/MyDrive/Colab_Notebooks/my_gpt2_model"

model.save_pretrained(model_path)
tokenizer.save_pretrained(model_path)

model.save_pretrained("/content/drive/MyDrive/Colab_Notebooks/my_gpt2_model")
tokenizer.save_pretrained("/content/drive/MyDrive/Colab_Notebooks/my_gpt2_model")

"""## Loading a Trained Model

In this step, I will load a previously trained GPT-2 model from Google Drive into Colab.

To do this, the code will copy the `pytorch_model.bin` and `config.json` files from a specific folder in my Google Drive to the Colab environment.

If I donâ€™t mention a specific folder, it will look for the files in the root directory of my Drive.

This allows me to reuse the trained model without training it again.
"""

model = GPT2LMHeadModel.from_pretrained(model_path)
tokenizer = GPT2Tokenizer.from_pretrained(model_path)

model.eval()

"""## Generate Text Using Your Trained GPT-2 Model

We use Hugging Face's `pipeline` to generate text from a starting prompt using the model we trained and saved.
"""

model = GPT2LMHeadModel.from_pretrained("sanjana2006-ngs/Text_Generation_with_GPT-2")
tokenizer = GPT2Tokenizer.from_pretrained("sanjana2006-ngs/Text_Generation_with_GPT-2")

generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

st.title("Custom GPT-2 Text Generator")
prompt = st.text_input("Enter your prompt", "Once upon a time")

if st.button("Generate"):
    result = generator(prompt, max_length=100, num_return_sequences=1)
    st.subheader("Generated Text:")
    st.write(result[0]["text"])

